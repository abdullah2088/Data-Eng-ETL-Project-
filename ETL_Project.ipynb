{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ifdT0TRpLY",
        "outputId": "0898013b-a0a2-460f-e459-da54d8f9cb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT * from Countries_by_GDP WHERE GDP_USD_billions >= 100\n",
            "          Country  GDP_USD_billions\n",
            "0   United States          26854.60\n",
            "1           China          19373.59\n",
            "2           Japan           4409.74\n",
            "3         Germany           4308.85\n",
            "4           India           3736.88\n",
            "..            ...               ...\n",
            "64          Kenya            118.13\n",
            "65         Angola            117.88\n",
            "66           Oman            104.90\n",
            "67      Guatemala            102.31\n",
            "68       Bulgaria            100.64\n",
            "\n",
            "[69 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Code for ETL operations on Country-GDP data\n",
        "\n",
        "# Importing the required libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "def extract(url, table_attribs):\n",
        "    ''' Extract GDP data from Wikipedia page and return as DataFrame. '''\n",
        "    page = requests.get(url).text\n",
        "    data = BeautifulSoup(page, 'html.parser')\n",
        "    df = pd.DataFrame(columns=table_attribs)\n",
        "\n",
        "    # Find GDP tables dynamically (Wikipedia tables with class \"wikitable\")\n",
        "    tables = data.find_all('table', {'class': 'wikitable'})\n",
        "    for table in tables:\n",
        "        rows = table.find_all('tr')\n",
        "        for row in rows:\n",
        "            col = row.find_all('td')\n",
        "            if len(col) != 0:\n",
        "                if col[0].find('a') is not None and '—' not in col[2].text:\n",
        "                    data_dict = {\n",
        "                        \"Country\": col[0].a.text.strip(),\n",
        "                        \"GDP_USD_millions\": col[2].text.strip()\n",
        "                    }\n",
        "                    df1 = pd.DataFrame(data_dict, index=[0])\n",
        "                    df = pd.concat([df, df1], ignore_index=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def transform(df):\n",
        "    ''' Convert GDP from string to float (Billions, 2 decimal places). '''\n",
        "    GDP_list = df[\"GDP_USD_millions\"].tolist()\n",
        "    GDP_list = [float(\"\".join(x.split(','))) for x in GDP_list]\n",
        "    GDP_list = [np.round(x/1000, 2) for x in GDP_list]  # Millions → Billions\n",
        "    df[\"GDP_USD_millions\"] = GDP_list\n",
        "    df = df.rename(columns={\"GDP_USD_millions\": \"GDP_USD_billions\"})\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_to_csv(df, csv_path):\n",
        "    ''' Save dataframe as CSV file. '''\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "\n",
        "def load_to_db(df, sql_connection, table_name):\n",
        "    ''' Save dataframe to database table. '''\n",
        "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)\n",
        "\n",
        "\n",
        "def run_query(query_statement, sql_connection):\n",
        "    ''' Run query on DB and print output. '''\n",
        "    print(query_statement)\n",
        "    query_output = pd.read_sql(query_statement, sql_connection)\n",
        "    print(query_output)\n",
        "\n",
        "\n",
        "def log_progress(message):\n",
        "    ''' Log progress messages with timestamp. '''\n",
        "    timestamp_format = '%Y-%m-%d-%H:%M:%S'  # Fixed format\n",
        "    now = datetime.now()\n",
        "    timestamp = now.strftime(timestamp_format)\n",
        "    with open(\"./etl_project_log.txt\", \"a\") as f:\n",
        "        f.write(timestamp + ' : ' + message + '\\n')\n",
        "\n",
        "\n",
        "# ------------------- MAIN ETL PIPELINE -------------------\n",
        "\n",
        "url = 'https://web.archive.org/web/20230902185326/https://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28nominal%29'\n",
        "table_attribs = [\"Country\", \"GDP_USD_millions\"]\n",
        "db_name = 'World_Economies.db'\n",
        "table_name = 'Countries_by_GDP'\n",
        "csv_path = './Countries_by_GDP.csv'\n",
        "\n",
        "log_progress('Preliminaries complete. Initiating ETL process')\n",
        "\n",
        "df = extract(url, table_attribs)\n",
        "log_progress('Data extraction complete. Initiating Transformation process')\n",
        "\n",
        "df = transform(df)\n",
        "log_progress('Data transformation complete. Initiating loading process')\n",
        "\n",
        "load_to_csv(df, csv_path)\n",
        "log_progress('Data saved to CSV file')\n",
        "\n",
        "sql_connection = sqlite3.connect(db_name)\n",
        "log_progress('SQL Connection initiated.')\n",
        "\n",
        "load_to_db(df, sql_connection, table_name)\n",
        "log_progress('Data loaded to Database as table. Running the query')\n",
        "\n",
        "query_statement = f\"SELECT * from {table_name} WHERE GDP_USD_billions >= 100\"\n",
        "run_query(query_statement, sql_connection)\n",
        "\n",
        "log_progress('Process Complete.')\n",
        "sql_connection.close()\n"
      ]
    }
  ]
}